---
title: "chini_proj"
output:
  word_document: default
  html_document: default
  pdf_document: default
date: "2024-10-03"
editor_options:
  markdown:
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Load necessary libraries
library(tidyverse)
library(caret)

# Load data
data <- read.csv("survey.csv")

# Examine data structure
str(data)
summary(data)
```

```{r}

head(data$Gender)
str(data$Gender)

```

```{r}
# Convert categorical columns to factors
data$Gender <- as.factor(data$Gender)
data$family_history <- as.factor(data$family_history)
data$treatment <- as.factor(data$treatment)
data$work_interfere <- as.factor(data$work_interfere)


```

```{r}
library(factoextra)

```

k-means

```{r}
# Define a mode imputation function
impute_mode <- function(x) {
  mode_val <- names(sort(table(x), decreasing = TRUE))[1]
  x[is.na(x)] <- mode_val
  return(x)
}

# Apply mode imputation to work_interfere
data$work_interfere <- impute_mode(data$work_interfere)

# Add "Unknown" category
# data$work_interfere <- as.character(data$work_interfere)
# data$work_interfere[is.na(data$work_interfere)] <- "Unknown"
# data$work_interfere <- as.factor(data$work_interfere)

# Verify missing values are handled

summary(data$work_interfere)

library(dplyr)

# Select and scale relevant features for clustering
data_scaled <- data %>%
  select(Age, work_interfere, family_history) %>%
  mutate_all(as.numeric) %>%  # Convert categorical variables to numeric
  scale()

library(cluster)
library(factoextra)

# Perform K-means clustering
set.seed(123)
kmeans_result <- kmeans(data_scaled, centers = 3)  # Change 'centers' as needed

# Add cluster assignments to the original dataset
data$Cluster <- as.factor(kmeans_result$cluster)

# Visualize the clusters
fviz_cluster(kmeans_result, data = data_scaled, geom = "point", ellipse.type = "euclid") +
  labs(title = "K-means Clustering Visualization")

data %>%
  group_by(Cluster) %>%
  summarize(
    Avg_Age = mean(Age),
    Work_Interfere_Mode = names(sort(table(work_interfere), decreasing = TRUE))[1],
    Family_History_Rate = mean(as.numeric(family_history == "Yes"))
  )

# Visualize Age distribution
boxplot(data$Age, main = "Age Distribution", horizontal = TRUE)

# Remove rows with extreme outliers
data <- data %>% filter(Age < 100)  # Assuming valid ages are below 100

# Re-scale and re-run clustering
data_scaled <- data %>%
  select(Age, work_interfere, family_history) %>%
  mutate_all(as.numeric) %>%
  scale()

kmeans_result <- kmeans(data_scaled, centers = 3)
data$Cluster <- as.factor(kmeans_result$cluster)

fviz_cluster(kmeans_result, data = data_scaled, geom = "point", ellipse.type = "euclid") +
  labs(title = "K-means Clustering Visualization")

#now validating the results..
# Compute silhouette scores
silhouette_score <- silhouette(kmeans_result$cluster, dist(data_scaled))

# Plot silhouette scores
plot(silhouette_score, main = "Silhouette Plot for K-means Clusters")


# Average silhouette width
cat("Average Silhouette Width:", mean(silhouette_score[, 3]), "\n")

# Summarize clusters
cluster_summary <- data %>%
  group_by(Cluster) %>%
  summarize(
    Avg_Age = mean(Age, na.rm = TRUE),
    Work_Interfere_Mode = names(sort(table(work_interfere), decreasing = TRUE))[1],
    Family_History_Rate = mean(as.numeric(family_history == "Yes"), na.rm = TRUE)
  )
print(cluster_summary)

```

##Explaination

1.  Clustering Visualization From the K-means clustering visualization:

Clusters: The dataset was divided into three distinct clusters (Cluster 1, Cluster 2, Cluster 3).
Each cluster is represented by a unique color (red, green, blue) and a different shape in the graph.
Principal Components (Dimensions): The X-axis (Dim1) and Y-axis (Dim2) are derived from Principal Component Analysis (PCA), which reduces the multi-dimensional data (Age, Work Interfere, Family History) into two dimensions to make the clustering results visually interpretable.
These components explain 36.8% (Dim1) and 33.9% (Dim2) of the variation in the data.
Observations:

Cluster Separation:

Clusters are moderately well-separated, with distinct groups forming along the principal dimensions.
However, there is some overlap, suggesting shared characteristics between certain groups.
Cluster 3 (blue) is more spread out, indicating higher variability within this group.
Outliers:

A few data points lie at the edges of their clusters or outside the ellipses, indicating potential outliers.
2.
Cluster Characteristics From the summary of clusters:

Cluster Avg_Age Work_Interfere_Mode Family_History_Rate 1 (Red) 2.03e+08 Sometimes 1 (100%) 2 (Green) 28.27 Sometimes 0 (0%) 3 (Blue) 33.75 Never 0 (0%) Key Insights:

Cluster 1:

Age: The Avg_Age value (2.03e+08) is clearly incorrect due to data issues during the calculation.
Likely, this resulted from an invalid entry (e.g., outliers or data scaling).
Work Interfere: The most frequent response is "Sometimes", indicating moderate workplace interference.
Family History: All individuals in this cluster have a family history of mental health issues (100%).
Interpretation: This cluster might represent employees with familial predisposition to mental health challenges and moderate workplace interference.
The data error in Age should be corrected for more precise analysis.

Cluster 2:

Age: The average age is 28.27, indicating this group consists of younger employees.
Work Interfere: "Sometimes" is the most frequent response, suggesting moderate workplace interference.
Family History: None of the individuals in this cluster have a family history of mental health issues (0%).
Interpretation: This cluster represents younger employees without a family history of mental health issues but who still experience some workplace interference.

Cluster 3:

Age: The average age is 33.75, representing slightly older employees compared to Cluster 2.
Work Interfere: "Never" is the most frequent response, suggesting minimal or no workplace interference.
Family History: None of the individuals in this cluster have a family history of mental health issues (0%).
Interpretation: This cluster represents older employees who do not have a family history of mental health issues and report no interference with their work.

Actionable Insights Cluster-Based Support:

Cluster 1 (High-risk group): This group may need targeted interventions because all members have a family history of mental health issues and moderate workplace interference.
Policies such as enhanced mental health resources or flexible work options could benefit this group.
Cluster 2 (Younger employees): Although younger and without a family history, this group still reports workplace interference.
This could be due to factors like work stress or lack of coping mechanisms, which could be addressed through wellness programs and stress management training.
Cluster 3 (Low-risk group): This group appears to be the least affected, with no family history and no reported interference.
Regular check-ins and awareness programs can maintain their well-being.
Data Cleaning:

The unusually high average age (2.03e+08) in Cluster 1 is a clear data anomaly, likely due to outliers or errors introduced during scaling or data entry.
Cleaning and re-running the analysis would provide more accurate results.
Further Analysis:

Explore additional factors such as gender, treatment, and company-provided benefits to deepen the understanding of these clusters.
Investigate the overlap in the clustering visualization to understand shared characteristics between certain groups.

## 

1.  Analyze Cluster Characteristics

Purpose:

Understand the profiles of individuals in each cluster to draw actionable insights.
This includes analyzing key variables (e.g., age, workplace interference, family history).

Steps:

```         
1.  Summarize Key Attributes for Each Cluster:
•   Use descriptive statistics to find the average, mode, or proportion of key attributes in each cluster.
```

```{r}
data %>%
  group_by(Cluster) %>%
  summarize(
    Avg_Age = mean(Age, na.rm = TRUE),
    Work_Interfere_Mode = names(sort(table(work_interfere), decreasing = TRUE))[1],
    Family_History_Rate = mean(as.numeric(family_history == "Yes"), na.rm = TRUE)
  )
```

2.  Visualize the Results: • Create bar plots or boxplots to visually compare clusters across key attributes.

```{r}
library(ggplot2)

ggplot(data, aes(x = Cluster, y = Age, fill = Cluster)) +
  geom_boxplot() +
  labs(title = "Age Distribution by Cluster", x = "Cluster", y = "Age")

ggplot(data, aes(x = Cluster, fill = work_interfere)) +
  geom_bar(position = "fill") +
  labs(title = "Work Interference by Cluster", x = "Cluster", y = "Proportion")
```

Association Rule Mining

Purpose:

Discover relationships between variables such as family history, workplace support, and treatment-seeking behavior.

Steps:

```         
1.  Prepare Data for Association Rule Mining:
•   Select relevant categorical columns.
•   Convert these columns to a transactions object.
```

```{r}
library(arules)
library(arulesViz)
library(dplyr)

# Select relevant columns and convert to factors
data_arules <- data |>
  select(family_history, treatment, care_options, work_interfere) |>
  mutate(across(everything(), as.factor))

# Convert to transactions
data_transactions <- as(data_arules, "transactions")
```

```         
2.  Generate Association Rules:
•   Use the apriori algorithm to find frequent patterns and rules.
```

```{r}
rules <- apriori(data_transactions, parameter = list(supp = 0.1, conf = 0.8))
inspect(head(rules))
plot(rules, method = "graph", control = list(type = "items"))
```

Insights from the Visualization

```         
1.  High-Support Nodes:
•   Nodes such as family_history=No and work_interfere=Never have larger sizes, indicating these conditions are more frequent in the dataset.
2.  Strong Associations:
•   Rules such as family_history=Yes → treatment=Yes have darker red edges, indicating a strong lift. This suggests that individuals with a family history of mental health issues are more likely to seek treatment.
•   Similarly, care_options=Yes → treatment=Yes indicates a strong link between the availability of mental health care options and seeking treatment.
3.  Weak Associations:
•   Edges with lighter colors have lower lift values, indicating weaker or less significant associations.
•   For example, work_interfere=Never → care_options=Not sure suggests a weaker relationship.
4.  Notable Combinations:
•   The rule family_history=Yes → treatment=Yes seems to be one of the most significant in the dataset, as indicated by its size and lift.
•   The absence of mental health care options (care_options=No) is associated with treatment=No, reinforcing the importance of workplace support.
```

Next Steps

```         
1.  Analyze Strong Rules:
•   Inspect and summarize the strongest association rules based on lift:    
```

```{r}
inspect(sort(rules, by = "lift")[1:5])  # View the top 5 rules by lift
```

```         
1.  LHS (Left-hand Side):
•   The condition or set of items in the rule (e.g., {family_history=No, work_interfere=Never}).
2.  RHS (Right-hand Side):
•   The consequence or outcome predicted by the rule (e.g., {treatment=No}).
3.  Metrics:
•   Support:
•   Proportion of rows in the dataset that satisfy both LHS and RHS.
•   Example: A support of 0.124 means 12.4% of the dataset satisfies the rule.
•   Confidence:
•   Proportion of rows satisfying LHS that also satisfy RHS.
•   Example: A confidence of 0.89 means 89% of individuals meeting the LHS condition also meet the RHS condition.
•   Lift:
•   Indicates how much more likely RHS is to occur when LHS is present compared to random chance.
•   Lift > 1 suggests a strong positive association.
•   Coverage:
•   Proportion of rows satisfying the LHS condition only.
```

Rule Insights

Rule 1

```         
•   Rule: {family_history=No, work_interfere=Never} → {treatment=No}
•   Support: 0.124 (12.4% of the dataset satisfies this rule).
•   Confidence: 0.89 (89% of individuals with no family history and no workplace interference did not seek treatment).
•   Lift: 1.80 (These individuals are 1.8 times more likely to not seek treatment compared to random chance).
```

Interpretation: • Employees with no family history and no workplace interference are significantly less likely to seek treatment.
This suggests that employees not directly affected by family history or workplace stress may undervalue mental health resources.

Rule 2

```         
•   Rule: {work_interfere=Never} → {treatment=No}
•   Support: 0.145 (14.5% of the dataset satisfies this rule).
•   Confidence: 0.86 (86% of individuals reporting no workplace interference did not seek treatment).
•   Lift: 1.74 (This behavior is 1.74 times more likely than random chance).
```

Interpretation: • Individuals with no workplace interference are highly unlikely to seek treatment.
This reinforces the importance of workplace mental health awareness campaigns for individuals who may not feel immediate interference.

Rule 3

```         
•   Rule: {family_history=Yes, care_options=Yes} → {treatment=Yes}
•   Support: 0.140 (14% of the dataset satisfies this rule).
•   Confidence: 0.84 (84% of individuals with a family history and access to care options sought treatment).
•   Lift: 1.67 (This is 1.67 times more likely than random chance).
```

Interpretation: • Employees with both a family history of mental health issues and access to care options are significantly more likely to seek treatment.
This highlights the importance of workplace-provided care options in encouraging treatment.

Rule 4

```         
•   Rule: {treatment=No, work_interfere=Never} → {family_history=No}
•   Support: 0.124 (12.4% of the dataset satisfies this rule).
•   Confidence: 0.85 (85% of individuals who did not seek treatment and reported no workplace interference also had no family history).
•   Lift: 1.40 (This is 1.4 times more likely than random chance).
```

Interpretation: • Employees without a family history are unlikely to seek treatment, especially if they also experience no workplace interference.
This suggests that these employees might perceive mental health treatment as unnecessary.

Rule 5

```         
•   Rule: {treatment=No, care_options=Not sure} → {family_history=No}
•   Support: 0.125 (12.5% of the dataset satisfies this rule).
•   Confidence: 0.83 (83% of individuals who did not seek treatment and were unsure about care options also had no family history).
•   Lift: 1.35 (This is 1.35 times more likely than random chance).
```

Interpretation: • Employees unsure about the availability of care options and without a family history are less likely to seek treatment.
This suggests that increasing awareness about mental health resources could improve treatment-seeking behavior.

Key Takeaways

```         
1.  Influence of Workplace Interference:
•   Employees reporting no workplace interference (work_interfere=Never) are consistently less likely to seek treatment. This indicates a need for proactive outreach to individuals who may not feel workplace stress but could still benefit from mental health resources.
2.  Role of Family History:
•   A family history of mental health issues significantly influences treatment-seeking behavior, especially when paired with access to care options.
3.  Impact of Care Options:
•   Employees with access to care options are more likely to seek treatment, highlighting the importance of making these options visible and accessible.
4.  Awareness and Education:
•   Employees who are “not sure” about care options are less likely to seek treatment, suggesting a need for better communication and awareness of available mental health resources.

3.  Visualize Subsets of Rules:
•   Visualize only the strongest rules or a specific subset for clarity:
```

```{r}
strong_rules <- subset(rules, lift > 1.5 & confidence > 0.8)
plot(strong_rules, method = "graph", control = list(type = "items"))
```

Actionable Recommendations

1.  Increase Awareness and Accessibility of Care Options

    • Why: Employees with access to care options are more likely to seek treatment.
    • How: • Create visible communication campaigns (e.g., posters, emails, newsletters) to raise awareness about the availability of mental health resources.
    • Provide easy access to mental health benefits through a centralized portal or HR contact point.

2.  Target High-Risk Groups (Family History)

    • Why: Employees with a family history of mental health issues are more likely to seek treatment and benefit from interventions.
    • How: • Offer tailored support programs, such as counseling sessions or stress management workshops, specifically for individuals with family history.
    • Introduce anonymous self-assessment tools to encourage at-risk employees to seek help.

3.  Address Employees with No Workplace Interference

    • Why: Employees reporting no interference are less likely to seek treatment, possibly due to lower perceived need for mental health resources.
    • How: • Educate employees about the importance of mental health maintenance, even for those who feel unaffected.
    • Integrate mental health training into broader wellness initiatives to normalize treatment-seeking behavior.

4.  Improve Communication Around Mental Health Resources

    • Why: Employees unsure about care options are less likely to seek treatment, reflecting a communication gap.
    • How: • Regularly update employees on available care options through webinars, town halls, and Q&A sessions.
    • Provide clear, concise brochures or guides outlining mental health resources and how to access them.

5.  Leverage Workplace Policies to Normalize Treatment

    • Why: Employees may hesitate to seek treatment due to stigma or fear of professional consequences.
    • How: • Create policies ensuring confidentiality for employees seeking mental health support.
    • Foster a supportive environment by training managers to recognize and address mental health concerns without judgment.

Integrating Recommendations into Classification Analysis Feature Engineering

Incorporate key insights and predictors into the dataset: Ensure categorical variables like family_history, work_interfere, and care_options are converted to factors:

```{r}
data$Cluster <- as.factor(kmeans_result$cluster)#	Add Cluster assignments from the clustering analysis as a feature
data <- data %>%
  mutate(
    family_history = as.factor(family_history),
    work_interfere = as.factor(work_interfere),
    care_options = as.factor(care_options),
    treatment = as.factor(treatment)  # Ensure treatment is the target variable
  )
#Split the data into training (70%) and testing (30%) sets
library(caret)
# Clean and consolidate Gender variable before splitting
data$Gender <- case_when(
  data$Gender == "cis male" ~ "Male",
  data$Gender == "cis female" ~ "Female",
  TRUE ~ "Other"
)
data$Gender <- as.factor(data$Gender)


# Split the data
trainIndex <- createDataPartition(data$treatment, p = 0.7, list = FALSE)
train <- data[trainIndex, ]
test <- data[-trainIndex, ]
```

#Logistic regression

```{r}

library(forcats)
# Train logistic regression model
log_model <- glm(
  treatment ~ family_history + work_interfere + care_options + Cluster + Age + Gender,
  data = train,
  family = "binomial"
)

# Summarize the model
summary(log_model)
# Match levels of Gender in test data to those in training data
test$Gender <- factor(test$Gender, levels = levels(train$Gender))
# Group unknown levels as "Other"
train$Gender <- fct_lump(train$Gender, n = 10)  # Keep the top 10 most common levels
test$Gender <- fct_lump(test$Gender, n = 10)

# Predict on the test data
log_predicted <- predict(log_model, test, type = "response")
log_predicted_class <- ifelse(log_predicted > 0.5, "Yes", "No")
log_predicted_class <- as.factor(log_predicted_class)
```

#Random forest

```{r}
install.packages("randomForest")
library(randomForest)
# Train random forest model
rf_model <- randomForest(
  treatment ~ family_history + work_interfere + care_options + Cluster + Age + Gender,
  data = train,
  ntree = 100
)

# Predict on the test data
rf_predicted <- predict(rf_model, test)
print(rf_predicted) 
```

Evaluate Model Performance

a.  Confusion Matrix

```{r}
library(caret)
# Logistic Regression
confusionMatrix(log_predicted_class, test$treatment)

# Random Forest
confusionMatrix(rf_predicted, test$treatment)
```

ROC curve and AUC

```{r}
# Install and load required packages
if (!require("ROCR")) install.packages("ROCR", repos = "http://cran.us.r-project.org")
library(ROCR)
library(dplyr)

# Step 1: Inspect Predictions and True Labels
# Ensure predictions and true labels are clean and consistent
print("Inspecting Predictions and True Labels...")
str(log_predicted_class)
table(log_predicted_class, useNA = "ifany")

str(test$treatment)
table(test$treatment, useNA = "ifany")

# Step 2: Clean Predictions
log_predicted_class <- as.character(log_predicted_class)
log_predicted_class[log_predicted_class == ""] <- NA  # Handle empty strings
log_predicted_class <- as.factor(log_predicted_class)
log_predicted_numeric <- as.numeric(log_predicted_class)

# Step 3: Clean True Labels
test$treatment <- as.character(test$treatment)
test$treatment[test$treatment == ""] <- NA  # Handle empty strings
test$treatment <- as.factor(test$treatment)
test_treatment_numeric <- as.numeric(test$treatment)

# Step 4: Handle Missing Values
if (any(is.na(log_predicted_numeric)) | any(is.na(test_treatment_numeric))) {
  print("Handling Missing Values in Predictions and True Labels...")
  complete_cases <- !is.na(log_predicted_numeric) & !is.na(test_treatment_numeric)
  log_predicted_numeric <- log_predicted_numeric[complete_cases]
  test_treatment_numeric <- test_treatment_numeric[complete_cases]
}

# Step 5: Check Data Consistency
print("Re-checking for Missing Values...")
cat("Missing values in predictions:", sum(is.na(log_predicted_numeric)), "\n")
cat("Missing values in true labels:", sum(is.na(test_treatment_numeric)), "\n")

# Step 6: Compute ROC and AUC
# Create prediction object for ROC analysis
pred <- prediction(log_predicted_numeric, test_treatment_numeric)

# Generate performance metrics for ROC
perf <- performance(pred, "tpr", "fpr")

# Plot ROC Curve
plot(perf, col = "blue", main = "ROC Curve for Logistic Regression")
abline(a = 0, b = 1, lty = 2, col = "red")  # Add diagonal reference line

# Calculate AUC
auc <- performance(pred, measure = "auc")
cat("Logistic Regression AUC:", auc@y.values[[1]], "\n")
```

Analyze Results and Incorporate Recommendations

a.  Interpret Feature Importance

    • Use feature importance to understand which predictors drive treatment-seeking behavior:

    ```{r}
    # Load necessary library
    library(randomForest)

    # Step 1: Train Random Forest Model with Importance
    rf_model <- randomForest(
      treatment ~ family_history + work_interfere + care_options + Cluster + Age + Gender,
      data = train,
      ntree = 100,
      importance = TRUE
    )

    # Step 2: Extract Feature Importance
    importance_values <- importance(rf_model)

    # Print the feature importance values
    cat("Feature Importance Values:\n")
    print(importance_values)

    # Step 3: Visualize Feature Importance
    varImpPlot(rf_model, main = "Feature Importance")
    ```

    #Wieghted average ensemble

    ```{r}
    # Combine Logistic Regression and Random Forest Predictions
    # Logistic regression predicted probabilities
    log_probs <- predict(log_model, newdata = test, type = "response")

    # Random forest predicted probabilities
    rf_probs <- predict(rf_model, newdata = test, type = "prob")[, 2]  # Probability for "Yes"

    # Weighted average of probabilities
    # Assign weights: 0.6 for logistic regression, 0.4 for random forest
    combined_probs <- 0.6 * log_probs + 0.4 * rf_probs

    # Convert combined probabilities to class predictions
    combined_predictions <- ifelse(combined_probs > 0.5, "Yes", "No")
    combined_predictions <- as.factor(combined_predictions)

    # Evaluate Ensemble Predictions
    # Confusion Matrix
    library(caret)
    conf_matrix_ensemble <- confusionMatrix(combined_predictions, test$treatment)
    print(conf_matrix_ensemble)

    # ROC Curve and AUC for Ensemble
    library(ROCR)
    pred_ensemble <- prediction(combined_probs, as.numeric(test$treatment == "Yes"))
    perf_ensemble <- performance(pred_ensemble, "tpr", "fpr")

    # Plot ROC Curve
    plot(perf_ensemble, col = "blue", main = "ROC Curve for Ensemble Model")
    abline(a = 0, b = 1, lty = 2, col = "red")  # Add diagonal reference line

    # Calculate AUC
    auc_ensemble <- performance(pred_ensemble, measure = "auc")
    cat("Ensemble Model AUC:", auc_ensemble@y.values[[1]], "\n")
    ```

    1.  Accuracy: • The ensemble model achieved an accuracy of 76.06%, which is an improvement compared to the standalone Logistic Regression and Random Forest models.
    2.  AUC (Area Under the ROC Curve): • The ensemble model produced an AUC of 0.797, reflecting strong predictive power. • This indicates that the ensemble model is better at distinguishing between individuals likely to seek treatment and those who are not.
    3.  Balanced Accuracy: • The ensemble’s balanced accuracy (76.03%) shows it performs well for both classes (Yes and No), addressing potential imbalances in the data.

```{=html}
<!-- -->
```
2.  Confusion Matrix Analysis

    • True Positives (TP): 135 cases were correctly classified as No (individuals who didn’t seek treatment).
    • True Negatives (TN): 151 cases were correctly classified as Yes (individuals who sought treatment).
    • False Positives (FP): 51 cases were incorrectly classified as Yes when they were actually No. • False Negatives (FN): 39 cases were incorrectly classified as No when they were actually Yes.

Metrics Derived:

```         
•   Sensitivity (Recall): 72.58%
•   The model correctly identified 72.58% of individuals who did not seek treatment (Class No).
•   Specificity: 79.47%
•   The model correctly identified 79.47% of individuals who sought treatment (Class Yes).
•   Positive Predictive Value (PPV): 77.59%
•   Among individuals predicted to seek treatment, 77.59% were correctly classified.
•   Negative Predictive Value (NPV): 74.75%
•   Among individuals predicted not to seek treatment, 74.75% were correctly classified.
```

Key Predictors Identified

Based on feature importance: 1.
Work Interference: • Individuals experiencing frequent (Often) or occasional (Sometimes) interference at work are more likely to seek treatment.
2.
Family History: • Having a family history of mental illness is a strong predictor of treatment-seeking behavior.
3.
Care Options: • The availability of care options at work (or lack thereof) significantly influences whether individuals seek treatment.
4.
Clusters: • The clustering feature captures behavioral groups, potentially adding nuance to the model.
5.
Age: • Surprisingly, the contribution of Age is minimal, as indicated by low importance scores.


6.
Gender: • Gender contributions appear insignificant (possibly due to inconsistent or unclear gender labels in the dataset).


```{r}
# Load necessary libraries
library(cluster)
library(factoextra)
library(dplyr)

# Preprocess the data
data_clustering <- data %>%
  select(Age, work_interfere, family_history) %>%
  mutate_all(as.numeric) %>%
  na.omit()  # Remove rows with NA values

# Scale the data
data_scaled <- scale(data_clustering)

# Perform K-means clustering
set.seed(123)
kmeans_result <- kmeans(data_scaled, centers = 3, nstart = 25)

# Add K-means cluster assignments to the dataset
data_clustering$KMeans_Cluster <- kmeans_result$cluster

# Perform Hierarchical Clustering
# Compute the dissimilarity matrix
dissimilarity_matrix <- dist(data_scaled, method = "euclidean")

# Perform hierarchical clustering using Ward's method
hierarchical_result <- hclust(dissimilarity_matrix, method = "ward.D2")

# Cut the dendrogram into 3 clusters
hierarchical_clusters <- cutree(hierarchical_result, k = 3)

# Add Hierarchical cluster assignments to the dataset
data_clustering$Hierarchical_Cluster <- hierarchical_clusters

# Compare Silhouette Scores
silhouette_kmeans <- silhouette(kmeans_result$cluster, dissimilarity_matrix)
silhouette_hierarchical <- silhouette(hierarchical_clusters, dissimilarity_matrix)

# Compute average silhouette scores
avg_silhouette_kmeans <- mean(silhouette_kmeans[, 3])
avg_silhouette_hierarchical <- mean(silhouette_hierarchical[, 3])

# Print Silhouette Comparison
cat("K-means Silhouette Score:", avg_silhouette_kmeans, "\n")
cat("Hierarchical Silhouette Score:", avg_silhouette_hierarchical, "\n")

# Visualize Clustering Results
# Plot dendrogram for hierarchical clustering
plot(hierarchical_result, main = "Dendrogram for Hierarchical Clustering", xlab = "", sub = "")

# Visualize K-means Clustering
fviz_cluster(kmeans_result, data = data_scaled, ellipse.type = "euclid") +
  labs(title = "K-means Clustering Visualization")

# Visualize Hierarchical Clustering
fviz_cluster(list(data = data_scaled, cluster = hierarchical_clusters), geom = "point") +
  labs(title = "Hierarchical Clustering Visualization")

```
Comparision of k means and Hierarchical Clustering:

Clusters: Represent groups of individuals with similar profiles based on Age, work_interfere, and family_history. For example:
Cluster 1 might represent younger individuals with moderate workplace interference and family history.
Cluster 2 might consist of older individuals with minimal workplace interference.
Outliers: The distant points might indicate individuals with unique profiles that don’t align well with the main groups.
Comparison: K-means appears to provide more distinct and compact clusters, making it a better choice for this analysis.

```{r}
# Apriori Algorithm
apriori_rules <- apriori(
  data_transactions,
  parameter = list(supp = 0.1, conf = 0.8)
)
apriori_rules_sorted <- sort(apriori_rules, by = "lift")

# Inspect Apriori Rules
cat("Top 5 Rules from Apriori:\n")
inspect(head(apriori_rules_sorted, 5))

# FP-Growth Algorithm
fp_growth_rules <- eclat(
  data_transactions,
  parameter = list(supp = 0.1, maxlen = 5)
)
fp_rules <- ruleInduction(
  fp_growth_rules,
  data_transactions,
  confidence = 0.8
)
fp_rules_sorted <- sort(fp_rules, by = "lift")

# Inspect FP-Growth Rules
cat("\nTop 5 Rules from FP-Growth:\n")
inspect(head(fp_rules_sorted, 5))

# Compare Key Metrics for Top 5 Rules
comparison_table <- data.frame(
  Algorithm = rep(c("Apriori", "FP-Growth"), each = 5),
  Rule = c(
    labels(head(apriori_rules_sorted, 5)),
    labels(head(fp_rules_sorted, 5))
  ),
  Support = c(
    quality(head(apriori_rules_sorted, 5))$support,
    quality(head(fp_rules_sorted, 5))$support
  ),
  Confidence = c(
    quality(head(apriori_rules_sorted, 5))$confidence,
    quality(head(fp_rules_sorted, 5))$confidence
  ),
  Lift = c(
    quality(head(apriori_rules_sorted, 5))$lift,
    quality(head(fp_rules_sorted, 5))$lift
  )
)

# Print Comparison Table
print(comparison_table)

# Visualize Rules from Each Algorithm
par(mfrow = c(2, 1))  # Set up for side-by-side plots
plot(apriori_rules_sorted, method = "scatterplot", measure = c("support", "lift"), shading = "confidence", main = "Apriori Rules")
plot(fp_rules_sorted, method = "scatterplot", measure = c("support", "lift"), shading = "confidence", main = "FP-Growth Rules")

```
Key Observations
Commonality in Rules
Both Apriori and FP-Growth identified the same top rules:
Rule 1: {family_history=No, work_interfere=Never} → {treatment=No} with support = 0.124, confidence = 0.891, and lift = 1.801.
Rule 3: {family_history=Yes, care_options=Yes} → {treatment=Yes} with support = 0.140, confidence = 0.842, and lift = 1.667.
The similarity in rules indicates that either algorithm is effective at discovering patterns in this dataset.
Metrics
Support: Measures how frequently the rule appears in the dataset.

Both algorithms found rules with the same support values, indicating they are equally robust for identifying frequent patterns.
Confidence: Measures the reliability of the rule (how often RHS is true when LHS is true).

Both algorithms provide identical confidence values for the top rules, suggesting no difference in rule quality.
Lift: Indicates the strength of the rule compared to random chance.

Lift values are also identical between Apriori and FP-Growth, reinforcing that the rules produced by both algorithms have similar strength.

